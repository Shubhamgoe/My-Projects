{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpAD3F9w7yW5TbVxqVjW3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhamgoe/My-Projects/blob/Reinforcement-Learning/RL_coding_quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bandit algorithm\n",
        "\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Initialize the bandit environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Hyperparameters\n",
        "epsilon = 0.1  # Exploration rate\n",
        "n_iterations = 1000  # Number of iterations\n",
        "\n",
        "# Initialize action-value estimates and counts\n",
        "Q = np.zeros(n_actions)  # Estimated value of actions\n",
        "N = np.zeros(n_actions)  # Number of times each action was selected\n",
        "\n",
        "# Bandit algorithm loop\n",
        "env.reset()\n",
        "for episode in range(n_iterations):\n",
        "    # Epsilon-greedy action selection\n",
        "    if np.random.uniform(0,1) < epsilon:\n",
        "        action = env.action_space.sample()  # Explore: Random action\n",
        "    else:\n",
        "        action = np.argmax(Q)  # Exploit: Action with max estimated value\n",
        "\n",
        "    # Interact with the environment\n",
        "    state, reward, done, _ = env.step(action)\n",
        "\n",
        "    # Update counts and estimated values\n",
        "    N[action] += 1\n",
        "    Q[action] += (reward - Q[action]) / N[action]  # Incremental update\n",
        "\n",
        "    # Reset environment if episode is done\n",
        "    if done:\n",
        "        env.reset()\n",
        "\n",
        "# Print results\n",
        "print(\"Action-Value Estimates (Q):\", Q)\n",
        "print(\"Action Selection Counts (N):\", N)\n",
        "\n",
        "# Determine the best action\n",
        "best_action = np.argmax(Q)\n",
        "print(f\"Best Action: {best_action} with Q-value: {Q[best_action]}\")\n"
      ],
      "metadata": {
        "id": "ne1cs64IbsC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UCB bandit algorithm\n",
        "\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Initialize the bandit environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Hyperparameters\n",
        "epsilon = 0.1  # Exploration rate\n",
        "n_iterations = 1000  # Number of iterations\n",
        "\n",
        "# Initialize action-value estimates and counts\n",
        "Q = np.zeros(n_actions)  # Estimated value of actions\n",
        "\n",
        "actions = [i for i in range(env.action_space.n)]\n",
        "N = [0 for a in actions]  # Number of times each action was selected\n",
        "# Bandit algorithm loop\n",
        "c = 0.1\n",
        "env.reset()\n",
        "t = 0\n",
        "done = False\n",
        "for episode in range(n_iterations):\n",
        "    # UCB action selection\n",
        "    action = np.argmax([Q[a]+c*(np.sqrt(np.log(t)/N[a]+1)) for a in actions])\n",
        "    N[action] += 1\n",
        "    # Interact with the environment\n",
        "    state, reward, done, _ = env.step(action)\n",
        "\n",
        "    # Update counts and estimated values\n",
        "    N[action] += 1\n",
        "    Q[action] += (reward - Q[action]) / N[action]  # Incremental update\n",
        "    t+=1\n",
        "    # Reset environment if episode is done\n",
        "    if done:\n",
        "        env.reset()\n",
        "\n",
        "# Print results\n",
        "print(\"Action-Value Estimates (Q):\", Q)\n",
        "print(\"Action Selection Counts (N):\", N)\n",
        "\n",
        "# Determine the best action\n",
        "best_action = np.argmax(Q)\n",
        "print(f\"Best Action: {best_action} with Q-value: {Q[best_action]}\")\n"
      ],
      "metadata": {
        "id": "FqQtJe3igdqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Bandit Algorithms\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Initialize the bandit environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "h = [0 for a in range(env.action_space.n)]\n",
        "alpha = 0.1\n",
        "avg_rewards = 0\n",
        "iterations = 1000\n",
        "done = False\n",
        "actions = [i for i in range(env.action_space.n)]\n",
        "env.reset()\n",
        "for iter in range(iterations):\n",
        "  policy = np.exp(h - np.max(h))/np.sum(np.exp(h - np.max(h)))\n",
        "  action = np.random.choice(actions,p=policy)\n",
        "  state,reward,done,_ = env.step(action)\n",
        "  avg_rewards += (reward - avg_rewards)/(iter+1)\n",
        "  for a in actions:\n",
        "    if action == a:\n",
        "      h[a] += alpha*(reward - avg_rewards)*(1-policy[a])\n",
        "    else:\n",
        "      h[a] -= alpha*(reward - avg_rewards)*(policy[a])\n",
        "  if done:\n",
        "    env.reset()\n",
        "\n",
        "# Print results\n",
        "print(\"Preferences (H):\", h)\n",
        "print(\"Softmax Probabilities (policy):\", policy)\n",
        "\n",
        "# Determine the best action\n",
        "best_action = np.argmax(policy)\n",
        "print(f\"Best Action: {best_action} with Probability: {policy[best_action]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jGXhI393kKk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# policy iteration\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "\n",
        "policy = [np.random.choice(a) for _ in s]\n",
        "value = [0.0 for i in s]\n",
        "\n",
        "gamma = 0.9  # Discount factor\n",
        "theta = 1e-8  # Convergence threshold\n",
        "\n",
        "\n",
        "def policy_evaluation(policy, value, gamma, theta):\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for state in s:\n",
        "        v = value[state]\n",
        "        action = policy[state]\n",
        "        value[state] = np.sum([prob*(reward+gamma*value[next_state]) for prob, next_state, reward, done in env.P[state][action]])\n",
        "        delta = max(delta,abs(v-value[state]))\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def policy_improvement(value, gamma):\n",
        "    policy_stable = True\n",
        "    for state in s:\n",
        "      old_action = policy[state]\n",
        "      policy[state] = np.argmax([np.sum([prob*(reward+gamma*value[next_state]) for prob, next_state, reward, done in env.P[state][action]]) for action in a])\n",
        "      if old_action != policy[state]:\n",
        "        policy_stable = False\n",
        "    return policy,policy_stable\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "  value = policy_evaluation(policy, value, gamma, theta)\n",
        "  policy,policy_stable = policy_improvement(value, gamma)\n",
        "  if policy_stable:\n",
        "    break\n",
        "\n",
        "print(\"Optimal Policy:\", policy)"
      ],
      "metadata": {
        "id": "SOeUKvw_508r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value iteration\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "policy = [np.random.choice(a) for i in s]\n",
        "value = [0.0 for i in s]\n",
        "\n",
        "gamma = 0.9  # Discount factor\n",
        "theta = 1e-8  # Convergence threshold\n",
        "\n",
        "\n",
        "def value_iteration(policy, value, gamma, theta):\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for state in s:\n",
        "        v = value[state]\n",
        "\n",
        "        value[state] = np.max([np.sum([prob*(reward+gamma*value[next_state]) for prob, next_state, reward, done in env.P[state][action]]) for action in a])\n",
        "        delta = max(delta,abs(v-value[state]))\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "value = value_iteration(policy, value, gamma, theta)\n",
        "for state in s:\n",
        "  policy[state] = np.argmax([np.sum([prob*(reward+gamma*value[next_state]) for prob, next_state, reward, done in env.P[state][action]]) for action in a])\n",
        "\n",
        "\n",
        "print(\"Optimal Policy:\", policy)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7cYvtUGU5aC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monte Carlo\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "\n",
        "\n",
        "# Finally now something can be done\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# create policy\n",
        "policy = [np.random.choice(a) for i in s]\n",
        "# state_action values\n",
        "Q = [[0 for j in a] for i in s]\n",
        "# returns\n",
        "returns = [[[] for j in a] for i in s]\n",
        "\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "for i in range(num_of_episodes):\n",
        "    state = env.reset()\n",
        "    episode_data = []\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy[state]\n",
        "      next_state,reward,done,_ = env.step(action)\n",
        "      episode_data.append((state,action,reward))\n",
        "      state = next_state\n",
        "    G = 0\n",
        "    for t in reversed(range(len(episode_data))):\n",
        "      state,action,reward = episode_data[t]\n",
        "      G = reward + gamma * G\n",
        "\n",
        "      if (state,action) not in [(x[0],x[1]) for x in episode_data[0:t]]:\n",
        "        returns[state][action].append(G)\n",
        "        Q[state][action] = np.mean(returns[state][action])\n",
        "        policy[state] = np.argmax(Q[state])\n",
        "\n",
        "\n",
        "print(policy)\n",
        "print(Q)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ASYnaeOp12OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monte Carlo (exploring start + epsilon greedy policy improvement)\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# policy\n",
        "policy = [[np.random.uniform(0,1) for j in a] for i in s]\n",
        "policy = [list(np.array(p)/np.sum(p)) for p in policy]\n",
        "print(policy)\n",
        "# state_action values\n",
        "Q = [[0 for j in a] for i in s]\n",
        "N = [[0 for j in a] for i in s]\n",
        "# returns\n",
        "# returns = [[[] for j in a] for i in s]\n",
        "\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "for i in range(num_of_episodes):\n",
        "    env.reset()\n",
        "    state = env.observation_space.sample()\n",
        "    episode_data = []\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = np.random.choice(a,p =policy[state])\n",
        "      next_state,reward,done,_ = env.step(action)\n",
        "      episode_data.append((state,action,reward))\n",
        "      state = next_state\n",
        "    G = 0\n",
        "    for t in reversed(range(len(episode_data))):\n",
        "      state,action,reward = episode_data[t]\n",
        "      G = reward + gamma * G\n",
        "      # print(G)\n",
        "      if (state,action) not in [(x[0],x[1]) for x in episode_data[0:t]]:\n",
        "        # returns[state][action].append(G)\n",
        "        N[state][action] += 1\n",
        "        Q[state][action] += (G - Q[state][action])/N[state][action]\n",
        "        # if(np.random.uniform(0,1)<epsilon):\n",
        "        #   policy[state] = np.random.choice(a)\n",
        "        # else:\n",
        "        #   policy[state] = np.argmax(Q[state])                       approach of deterministic policy\n",
        "        for action in a:\n",
        "          if(action == np.argmax(Q[state])):\n",
        "            policy[state][action] = 1 - epsilon + epsilon/len(a)\n",
        "          else:\n",
        "            policy[state][action] = epsilon/len(a)\n",
        "            print(policy[state][action])\n",
        "                                                                      #approach of epsilon soft policy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(policy)\n",
        "# print(Q)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQgn2u4v-TSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Off policy prediction Monte Carlo\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# policy\n",
        "target_policy = [[np.random.uniform(0,1) for j in a] for i in s]\n",
        "target_policy = [list(np.array(p)/np.sum(p)) for p in target_policy]\n",
        "# print(policy)\n",
        "# state_action values\n",
        "Q = [[0 for j in a] for i in s]\n",
        "C = [[0 for j in a] for i in s]\n",
        "# target_policy[state] = np.argmax(Q[state])\n",
        "# returns\n",
        "# returns = [[[] for j in a] for i in s]\n",
        "\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "for i in range(num_of_episodes):\n",
        "    env.reset()\n",
        "    behaviour_policy = [[1/len(a) for j in a] for i in s]\n",
        "    state = env.observation_space.sample()\n",
        "    episode_data = []\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = np.random.choice(a,p =behaviour_policy[state])\n",
        "      next_state,reward,done,_ = env.step(action)\n",
        "      episode_data.append((state,action,reward))\n",
        "      state = next_state\n",
        "    G = 0\n",
        "    W = 1\n",
        "    for t in reversed(range(len(episode_data))):\n",
        "      state,action,reward = episode_data[t]\n",
        "      G = reward + gamma * G\n",
        "      # print(G)\n",
        "\n",
        "      C[state][action] += W\n",
        "      Q[state][action] += (G - Q[state][action])/C[state][action]\n",
        "      for action in a:\n",
        "        if(action == np.argmax(Q[state])):\n",
        "          target_policy[state][action] = 1 - epsilon + epsilon/len(a)\n",
        "        else:\n",
        "          target_policy[state][action] = epsilon/len(a)\n",
        "      # target_policy[state] = np.argmax(Q[state])                     if target_policy is deterministic\n",
        "      W = W * (target_policy[state][action]/behaviour_policy[state][action])\n",
        "      # W = W * (1/behaviour_policy[state][action])                    if target_policy is deterministic\n",
        "\n",
        "                                                                    #approach of epsilon soft policy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(policy)\n",
        "# print(Q)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RgiCxiu0sTBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q learning\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# policy\n",
        "policy = [[np.random.uniform(0,1) for j in a] for i in s]\n",
        "policy = [list(np.array(p)/np.sum(p)) for p in policy]\n",
        "# print(policy)\n",
        "# state_action values\n",
        "Q = [[0 for j in a] for i in s]\n",
        "N = [[0 for j in a] for i in s]\n",
        "# returns\n",
        "# returns = [[[] for j in a] for i in s]\n",
        "\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "for i in range(num_of_episodes):\n",
        "    env.reset()\n",
        "    state = env.observation_space.sample()\n",
        "    # episode_data = []\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = np.random.choice(a,p=policy[state])\n",
        "      next_state,reward,done,_ = env.step(action)\n",
        "      N[state][action] += 1\n",
        "      next_action = np.argmax(Q[next_state])\n",
        "      Q[state][action] += (reward + gamma*Q[next_state][next_action] - Q[state][action])/N[state][action]\n",
        "      # if(np.random.uniform(0,1)<epsilon):\n",
        "      #   policy[state] = np.random.choice(a)\n",
        "      # else:\n",
        "      #   policy[state] = np.argmax(Q[state])                       approach of deterministic policy\n",
        "      for action in a:\n",
        "        if(action == np.argmax(Q[state])):\n",
        "          policy[state][action] = 1 - epsilon + epsilon/len(a)\n",
        "        else:\n",
        "          policy[state][action] = epsilon/len(a)\n",
        "                                                                    #approach of epsilon soft policy\n",
        "      state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(policy)\n",
        "# print(Q)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XVVyPid8VtCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SARSA\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# policy\n",
        "policy = [[np.random.uniform(0,1) for j in a] for i in s]\n",
        "policy = [list(np.array(p)/np.sum(p)) for p in policy]\n",
        "# print(policy)\n",
        "# state_action values\n",
        "Q = [[0 for j in a] for i in s]\n",
        "N = [[0 for j in a] for i in s]\n",
        "# returns\n",
        "# returns = [[[] for j in a] for i in s]\n",
        "\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "for i in range(num_of_episodes):\n",
        "    env.reset()\n",
        "    state = env.observation_space.sample()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = np.random.choice(a,p=policy[state])\n",
        "      next_state,reward,done,_ = env.step(action)\n",
        "      for action in a:\n",
        "        if(action == np.argmax(Q[next_state])):\n",
        "          policy[next_state][action] = 1 - epsilon + epsilon/len(a)\n",
        "        else:\n",
        "          policy[next_state][action] = epsilon/len(a)\n",
        "      N[state][action] += 1\n",
        "      next_action = np.random.choice(a,p=policy[next_state])\n",
        "      Q[state][action] += (reward + gamma*Q[next_state][next_action] - Q[state][action])/N[state][action]\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(policy)\n",
        "# print(Q)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "suAnhXUZjUUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPECTED SARSA\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# policy\n",
        "policy = [[np.random.uniform(0,1) for j in a] for i in s]\n",
        "policy = [list(np.array(p)/np.sum(p)) for p in policy]\n",
        "# print(policy)\n",
        "# state_action values\n",
        "Q = [[0 for j in a] for i in s]\n",
        "N = [[0 for j in a] for i in s]\n",
        "# returns\n",
        "# returns = [[[] for j in a] for i in s]\n",
        "\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "for i in range(num_of_episodes):\n",
        "    env.reset()\n",
        "    state = env.observation_space.sample()\n",
        "    # episode_data = []\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = np.random.choice(a,p=policy[state])\n",
        "      next_state,reward,done,_ = env.step(action)\n",
        "      # returns[state][action].append(G)\n",
        "      for action in a:\n",
        "        if(action == np.argmax(Q[next_state])):\n",
        "          policy[next_state][action] = 1 - epsilon + epsilon/len(a)\n",
        "        else:\n",
        "          policy[next_state][action] = epsilon/len(a)\n",
        "      N[state][action] += 1\n",
        "      # next_action = np.random.choice(a,p=policy[next_state])\n",
        "      target = np.sum([policy[next_state][i]*Q[next_state][i] for i in a])\n",
        "      Q[state][action] += (reward + gamma*target - Q[state][action])/N[state][action]\n",
        "\n",
        "      state = next_state\n",
        "    print(policy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(Q)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oKIrt_wGo0hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double Q learning\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# policy\n",
        "policy = [[np.random.uniform(0,1) for j in a] for i in s]\n",
        "policy = [list(np.array(p)/np.sum(p)) for p in policy]\n",
        "Q1 = [[0 for j in a] for i in s]\n",
        "Q2 = [[0 for j in a] for i in s]\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "alpha = 0.1\n",
        "for i in range(num_of_episodes):\n",
        "    env.reset()\n",
        "    state = env.observation_space.sample()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = np.random.choice(a,p=policy[state])\n",
        "      next_state,reward,done,_ = env.step(action)\n",
        "      if np.random.uniform(0,1) < 0.5:\n",
        "         next_action = np.argmax(Q1[next_state])\n",
        "         Q1[state][action] += alpha*(reward + gamma*Q2[next_state][next_action] - Q1[state][action])\n",
        "      else:\n",
        "         next_action = np.argmax(Q2[next_state])\n",
        "         Q2[state][action] += alpha*(reward + gamma*Q1[next_state][next_action] - Q2[state][action])\n",
        "      for action in a:\n",
        "        if(action == np.argmax([Q1[state][j]+Q2[state][j] for j in a])):\n",
        "          policy[state][action] = 1 - epsilon + epsilon/len(a)\n",
        "        else:\n",
        "          policy[state][action] = epsilon/len(a)\n",
        "      state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(policy)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9WDSKg7eN0Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n step SARSA\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# policy\n",
        "policy = [[np.random.uniform(0,1) for j in a] for i in s]\n",
        "policy = [list(np.array(p)/np.sum(p)) for p in policy]\n",
        "# print(policy)\n",
        "# state_action values\n",
        "Q = [[0 for j in a] for i in s]\n",
        "N = [[0 for j in a] for i in s]\n",
        "# returns\n",
        "# returns = [[[] for j in a] for i in s]\n",
        "\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "n = 3\n",
        "for i in range(num_of_episodes):\n",
        "    state = env.observation_space.sample()\n",
        "    episode_data = []\n",
        "    done = False\n",
        "    t = 0\n",
        "    while True:\n",
        "      if(not done):\n",
        "        action = np.random.choice(a,p=policy[state])\n",
        "        next_state,reward,done,_ = env.step(action)\n",
        "        # returns[state][action].append(G)\n",
        "        next_action = np.random.choice(a,p=policy[next_state])\n",
        "        episode_data.append((state,action,reward,next_state,next_action))\n",
        "        state = next_state\n",
        "      if(len(episode_data) is not 0 and len(episode_data)==t):\n",
        "        break\n",
        "      if len(episode_data) >= n:\n",
        "        state_up = episode_data[t][0]\n",
        "        action = episode_data[t][1]\n",
        "        N[state_up][action] += 1\n",
        "        tot_discounted_reward = 0\n",
        "        for h in range(t,len(episode_data)):\n",
        "          rewardi = episode_data[h][2]\n",
        "          tot_discounted_reward = (gamma)*tot_discounted_reward + rewardi\n",
        "        if (len(episode_data) - t == n):\n",
        "          nth_state = episode_data[len(episode_data)-1][3]\n",
        "          nth_action = episode_data[len(episode_data)-1][4]\n",
        "          may_vary = Q[nth_state][nth_action]\n",
        "        else:\n",
        "          may_vary = 0\n",
        "\n",
        "        Q[state_up][action] += (tot_discounted_reward + (gamma**n)*may_vary - Q[state_up][action])/N[state_up][action]\n",
        "        for action in a:\n",
        "          if(action == np.argmax(Q[state])):\n",
        "            policy[state_up][action] = 1 - epsilon + epsilon/len(a)\n",
        "          else:\n",
        "            policy[state_up][action] = epsilon/len(a)\n",
        "        t += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(policy)\n",
        "# print(Q)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CsnOWH3QsAlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n step Expected SARSA\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "# state space\n",
        "s = [i for i in range(env.observation_space.n)]\n",
        "# action space\n",
        "a = [i for i in range(env.action_space.n)]\n",
        "# policy\n",
        "policy = [[np.random.uniform(0,1) for j in a] for i in s]\n",
        "policy = [list(np.array(p)/np.sum(p)) for p in policy]\n",
        "# print(policy)\n",
        "# state_action values\n",
        "Q = [[0 for j in a] for i in s]\n",
        "N = [[0 for j in a] for i in s]\n",
        "\n",
        "\n",
        "num_of_episodes = 1000\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "n = 3\n",
        "for i in range(num_of_episodes):\n",
        "    state = env.observation_space.sample()\n",
        "    episode_data = []\n",
        "    done = False\n",
        "    t = 0\n",
        "    while True:\n",
        "      if(not done):\n",
        "        action = np.random.choice(a,p=policy[state])\n",
        "        next_state,reward,done,_ = env.step(action)\n",
        "        next_action = np.random.choice(a,p=policy[next_state])\n",
        "        episode_data.append((state,action,reward,next_state,next_action))\n",
        "        state = next_state\n",
        "      if(len(episode_data) is not 0 and len(episode_data)==t):\n",
        "        break\n",
        "      if len(episode_data) >= n:\n",
        "        state_up = episode_data[t][0]\n",
        "        action = episode_data[t][1]\n",
        "        N[state_up][action] += 1\n",
        "        tot_discounted_reward = 0\n",
        "        for h in range(t,len(episode_data)):\n",
        "          rewardi = episode_data[h][2]\n",
        "          tot_discounted_reward = (gamma**(h-t))*tot_discounted_reward + rewardi\n",
        "        if (len(episode_data) - t == n):\n",
        "          nth_state = episode_data[len(episode_data)-1][3]\n",
        "          nth_action = episode_data[len(episode_data)-1][4]\n",
        "          may_vary = np.sum([policy[nth_state][i]*Q[nth_state][i] for i in a])\n",
        "        else:\n",
        "          may_vary = 0\n",
        "\n",
        "        Q[state_up][action] += (tot_discounted_reward + (gamma**n)*may_vary - Q[state_up][action])/N[state_up][action]\n",
        "        for action in a:\n",
        "          if(action == np.argmax(Q[state])):\n",
        "            policy[state][action] = 1 - epsilon + epsilon/len(a)\n",
        "          else:\n",
        "            policy[state][action] = epsilon/len(a)\n",
        "        t += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(policy)\n",
        "# print(Q)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "esuLEcR9cQDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}